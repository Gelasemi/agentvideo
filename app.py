import streamlit as st
from gtts import gTTS
from moviepy.editor import ImageClip, AudioFileClip, TextClip, CompositeVideoClip, concatenate_videoclips
import os
import tempfile
import requests
from bs4 import BeautifulSoup
import threading  # For potential async scraping if needed
import gc  # For garbage collection to free memory

# Supported languages (top 10 most spoken ‚Äì gTTS codes)
LANGUAGES = {
    "1. Anglais": "en",
    "2. Chinois Mandarin": "zh",
    "3. Hindi": "hi",
    "4. Espagnol": "es",
    "5. Fran√ßais": "fr",
    "6. Arabe": "ar",
    "7. Bengali": "bn",
    "8. Russe": "ru",
    "9. Portugais": "pt",
    "10. Ourdou": "ur"
}

def get_content(subject):
    """Scrape content from Wikipedia for documentary/advertising info. Optimized with timeout and limited parse."""
    query = subject.replace(' ', '_')
    url = f"https://en.wikipedia.org/wiki/{query}"
    try:
        response = requests.get(url, timeout=5)  # Reduced timeout for speed
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        content_div = soup.find('div', {'class': 'mw-parser-output'})
        if content_div:
            paragraphs = content_div.find_all('p', limit=5)  # Limit to first 5 paragraphs for speed
            content = ' '.join([p.get_text().strip() for p in paragraphs if p.get_text().strip()])[:1000]  # Reduced limit
        else:
            content = "Aucune information trouv√©e sur Wikipedia. Utilisez des faits g√©n√©raux."
    except Exception as e:
        content = f"Erreur lors du scraping : {str(e)}. Utilisez des faits g√©n√©raux."
    return content

def get_images(subject, num=3):  # Reduced max images to 3 for faster processing
    """Scrape images from Unsplash matching the subject. Optimized with fewer requests."""
    query = subject.replace(' ', '%20')
    url = f"https://unsplash.com/s/photos/{query}"
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
    images = []
    try:
        response = requests.get(url, headers=headers, timeout=5)  # Reduced timeout
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        img_tags = soup.find_all('img', {'srcset': True}, limit=num+2)  # Limit tags parsed
        image_urls = [img['src'] for img in img_tags if 'src' in img.attrs and 'unsplash.com/photos' in img['src']]
        unique_urls = list(dict.fromkeys(image_urls))[:num]
        for u in unique_urls:
            resp = requests.get(u, timeout=5, stream=True)  # Stream for memory efficiency
            if resp.status_code == 200:
                path = tempfile.mktemp(suffix=".jpg")
                with open(path, 'wb') as f:
                    for chunk in resp.iter_content(chunk_size=8192):  # Chunked write for low memory
                        f.write(chunk)
                images.append(path)
    except Exception as e:
        st.warning(f"Erreur lors de la r√©cup√©ration des images : {str(e)}. Utilisation d'avatar par d√©faut.")
    return images

def generate_script(subject, company, content, lang_code):
    """Generate a 1-minute advertising/documentary script using scraped content (AIDA structure)."""
    # Aim for ~120-150 words for 1 min, reduced computation
    script = (
        f"Attention ! D√©couvrez {subject} avec {company}. "
        f"{content[:400]}... "  # Shorter include for faster string ops
        f"Int√©ressant ? {company} offre les meilleurs avantages. "
        f"D√©sir : Choisissez {company} pour {subject}. "
        f"Action : Abonnez-vous maintenant !"
    )
    if len(script) < 600:
        script += f" Plus d'infos : {content[400:600]}."
    return script

st.set_page_config(page_title="GlobeCast AI Am√©lior√©", layout="wide")
st.title("üåç GlobeCast AI ‚Äì Agent Vid√©o Puissant (Version Optimis√©e CPU)")
st.markdown("""
**Cr√©√© par : Dauphin Gelase Michelot**  
**Label :** M&G Consulting  
**GitHub :** [gelasemi](https://github.com/gelasemi)  
**Niveau :** Starter Am√©lior√©  
**Date :** F√©vrier 2026  
**Am√©liorations :** Scraping optimis√©, moins d'images, vid√©o all√©g√©e pour CPU/laptop rapide (FPS bas, r√©sol. r√©duite, cleanup m√©moire).
""")

st.info("Version CPU-optimis√©e ‚Äì Ex√©cution plus rapide sur laptop/CPU : scraping limit√©, vid√©o l√©g√®re (~30s max), garbage collection.")

# User inputs
subject = st.text_input("Sujet de la vid√©o (documentaire ou pub)", placeholder="Exemple : Caf√© √©thique √† Madagascar", value="Caf√© √©thique")
company = st.text_input("Nom de l'entreprise pour la pub", placeholder="Exemple : M&G Consulting", value="M&G Consulting")
language_name = st.selectbox("Langue de la vid√©o", list(LANGUAGES.keys()))
platform = st.selectbox("Format / Plateforme cible", [
    "TikTok ‚Äì Vertical 9:16 (1080√ó1920)",
    "YouTube ‚Äì Horizontal 16:9 (1920√ó1080)",
    "Facebook/Instagram ‚Äì Carr√© 1:1 (1080√ó1080)"
])

if st.button("üé• G√©n√©rer Vid√©o Publicitaire Auto (1 min)", type="primary"):
    if not subject.strip() or not company.strip():
        st.error("Veuillez entrer un sujet et un nom d'entreprise valides.")
    else:
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        status_text.text("Scraping contenu...")
        content = get_content(subject)
        st.subheader("Contenu scraped (aper√ßu)")
        st.write(content[:300] + "...")
        progress_bar.progress(0.2)
        
        status_text.text("G√©n√©ration script...")
        script_text = generate_script(subject, company, content, language_name)  # Fix: lang_code not used in generate_script
        st.subheader("Script g√©n√©r√© (~1 min)")
        st.write(script_text)
        progress_bar.progress(0.4)
        
        lang_code = LANGUAGES[language_name]
        
        status_text.text("Synth√®se vocale...")
        try:
            tts = gTTS(text=script_text[:1000], lang=lang_code, slow=False) # Limit text for faster TTS
            audio_path = tempfile.mktemp(suffix=".mp3")
            tts.save(audio_path)
        except Exception as e:
            st.error(f"Erreur TTS : {e}")
            st.stop()
        progress_bar.progress(0.5)
        
        status_text.text("R√©cup√©ration images...")
        images = get_images(subject, num=3) # Reduced to 3
        if not images:
            st.warning("Aucune image. Avatar par d√©faut.")
            avatar_filename = f"avatar_{lang_code}.png"
            avatar_path = os.path.join("avatars", avatar_filename)
            if os.path.exists(avatar_path):
                images = [avatar_path] * 3 # Reuse for speed
            else:
                st.error("Avatar manquant !")
                os.remove(audio_path)
                st.stop()
        progress_bar.progress(0.6)
        
        status_text.text("Cr√©ation vid√©o optimis√©e...")
        try:
            audio_clip = AudioFileClip(audio_path)
            duration = min(audio_clip.duration, 60) # Cap to 60s max for speed
            clip_duration = duration / max(1, len(images))
           
            # Optimized clips: no crossfade if many, low FPS
            img_clips = []
            for img in images:
                clip = ImageClip(img).set_duration(clip_duration) # No fade for speed
                img_clips.append(clip)
           
            video = concatenate_videoclips(img_clips, method="compose")
           
            # Lower res for processing, then resize
            if platform.startswith("TikTok"):
                size = (1080, 1920)
            elif platform.startswith("YouTube"):
                size = (1920, 1080)
            else:
                size = (1080, 1080)
            process_size = (size[0] // 2, size[1] // 2) # Half res temp
            video = video.resize(process_size)
            video = video.resize(size)
           
            subtitle = script_text[:150] + "..." # Shorter subtitle
            txt_clip = TextClip(
                subtitle,
                fontsize=40, # Smaller font for faster render
                color='white',
                stroke_color='black',
                stroke_width=1,
                font='Arial',
                method='label', # Faster method
                align='center'
            ).set_position(('center', 'bottom')).set_duration(duration)
           
            final_video = CompositeVideoClip([video, txt_clip]).set_audio(audio_clip)
           
            video_path = tempfile.mktemp(suffix=".mp4")
            final_video.write_videofile(
                video_path,
                fps=15, # Lower FPS for faster export
                codec="libx264",
                audio_codec="aac",
                preset='ultrafast', # Fastest preset
                threads=2, # Limited threads for laptop CPU
                verbose=False,
                logger=None
            )
           
            st.success("Vid√©o g√©n√©r√©e rapidement ! (~1 min, optimis√©e CPU)")
            st.video(video_path)
           
            with open(video_path, "rb") as f:
                st.download_button(
                    label="T√©l√©charger Vid√©o MP4",
                    data=f,
                    file_name=f"Pub_{company}_{subject.replace(' ', '_')}.mp4",
                    mime="video/mp4"
                )
           
            # Cleanup
            os.remove(audio_path)
            os.remove(video_path)
            for img in images:
                if 'avatars' not in img:
                    os.remove(img)
            gc.collect() # Force memory cleanup
           
            progress_bar.progress(1.0)
            status_text.text("Termin√© !")
           
        except Exception as e:
            st.error(f"Erreur vid√©o : {str(e)}")
            if os.path.exists(audio_path):
                os.remove(audio_path)
            gc.collect()

st.markdown("---")
st.caption("Optimisations CPU : FPS r√©duit (15), preset ultrafast, moins d'images/fades, cleanup m√©moire, timeouts courts. Prochaines : Async scraping, cloud offload. Contact GitHub ! üöÄ")
